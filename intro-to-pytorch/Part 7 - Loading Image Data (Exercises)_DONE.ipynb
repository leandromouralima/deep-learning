{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 7 - Loading Image Data (Exercises)_DONE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azlxzusX40x4",
        "colab_type": "text"
      },
      "source": [
        "# Loading Image Data\n",
        "\n",
        "So far we've been working with fairly artificial datasets that you wouldn't typically be using in real projects. Instead, you'll likely be dealing with full-sized images like you'd get from smart phone cameras. In this notebook, we'll look at how to load images and use them to train neural networks.\n",
        "\n",
        "We'll be using a [dataset of cat and dog photos](https://www.kaggle.com/c/dogs-vs-cats) available from Kaggle. Here are a couple example images:\n",
        "\n",
        "<img src='https://github.com/leandromouralima/deep-learning/blob/master/intro-to-pytorch/assets/dog_cat.png?raw=1'>\n",
        "\n",
        "We'll use this dataset to train a neural network that can differentiate between cats and dogs. These days it doesn't seem like a big accomplishment, but five years ago it was a serious challenge for computer vision systems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIAWQshb40x8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "1b1dc40b-f175-49ff-cc7c-05dfd2888fe4"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import helper\n",
        "\n",
        "!wget -nc https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip\n",
        "!unzip -f Cat_Dog_data.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-25 00:02:01--  https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.8.245\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.8.245|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 580495262 (554M) [application/zip]\n",
            "Saving to: ‘Cat_Dog_data.zip’\n",
            "\n",
            "Cat_Dog_data.zip    100%[===================>] 553.60M  72.0MB/s    in 8.7s    \n",
            "\n",
            "2019-06-25 00:02:10 (63.6 MB/s) - ‘Cat_Dog_data.zip’ saved [580495262/580495262]\n",
            "\n",
            "Archive:  Cat_Dog_data.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FtgWxhT8ZuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip Cat_Dog_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffz6-49d40yE",
        "colab_type": "text"
      },
      "source": [
        "The easiest way to load image data is with `datasets.ImageFolder` from `torchvision` ([documentation](http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder)). In general you'll use `ImageFolder` like so:\n",
        "\n",
        "```python\n",
        "dataset = datasets.ImageFolder('path/to/data', transform=transform)\n",
        "```\n",
        "\n",
        "where `'path/to/data'` is the file path to the data directory and `transform` is a list of processing steps built with the [`transforms`](http://pytorch.org/docs/master/torchvision/transforms.html) module from `torchvision`. ImageFolder expects the files and directories to be constructed like so:\n",
        "```\n",
        "root/dog/xxx.png\n",
        "root/dog/xxy.png\n",
        "root/dog/xxz.png\n",
        "\n",
        "root/cat/123.png\n",
        "root/cat/nsdf3.png\n",
        "root/cat/asd932_.png\n",
        "```\n",
        "\n",
        "where each class has it's own directory (`cat` and `dog`) for the images. The images are then labeled with the class taken from the directory name. So here, the image `123.png` would be loaded with the class label `cat`. You can download the dataset already structured like this [from here](https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip). I've also split it into a training set and test set.\n",
        "\n",
        "### Transforms\n",
        "\n",
        "When you load in the data with `ImageFolder`, you'll need to define some transforms. For example, the images are different sizes but we'll need them to all be the same size for training. You can either resize them with `transforms.Resize()` or crop with `transforms.CenterCrop()`, `transforms.RandomResizedCrop()`, etc. We'll also need to convert the images to PyTorch tensors with `transforms.ToTensor()`. Typically you'll combine these transforms into a pipeline with `transforms.Compose()`, which accepts a list of transforms and runs them in sequence. It looks something like this to scale, then crop, then convert to a tensor:\n",
        "\n",
        "```python\n",
        "transform = transforms.Compose([transforms.Resize(255),\n",
        "                                 transforms.CenterCrop(224),\n",
        "                                 transforms.ToTensor()])\n",
        "\n",
        "```\n",
        "\n",
        "There are plenty of transforms available, I'll cover more in a bit and you can read through the [documentation](http://pytorch.org/docs/master/torchvision/transforms.html). \n",
        "\n",
        "### Data Loaders\n",
        "\n",
        "With the `ImageFolder` loaded, you have to pass it to a [`DataLoader`](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). The `DataLoader` takes a dataset (such as you would get from `ImageFolder`) and returns batches of images and the corresponding labels. You can set various parameters like the batch size and if the data is shuffled after each epoch.\n",
        "\n",
        "```python\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "```\n",
        "\n",
        "Here `dataloader` is a [generator](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/). To get data out of it, you need to loop through it or convert it to an iterator and call `next()`.\n",
        "\n",
        "```python\n",
        "# Looping through it, get a batch on each loop \n",
        "for images, labels in dataloader:\n",
        "    pass\n",
        "\n",
        "# Get one batch\n",
        "images, labels = next(iter(dataloader))\n",
        "```\n",
        " \n",
        ">**Exercise:** Load images from the `Cat_Dog_data/train` folder, define a few transforms, then build the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEOvHFJA40yG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b18d6be5-67a3-4294-f615-774e140c1d80"
      },
      "source": [
        "data_dir = 'Cat_Dog_data/train'\n",
        "\n",
        "# TODO: compose transforms here\n",
        "transform = transforms.Compose([transforms.Resize(255),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor()])\n",
        "\n",
        "# TODO: create the ImageFolder\n",
        "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
        "\n",
        "# TODO: use the ImageFolder dataset to create the DataLoader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "type(dataloader)\n",
        "len(dataloader)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "704"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZLlPSJV8Vif",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoApkj1V9YoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import helper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enVKTksk9Yau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce82PK2T40yO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "3c349c69-d65d-48c5-9674-7db3adc51146"
      },
      "source": [
        "# Run this to test your data loader\n",
        "import helper\n",
        "images, labels = next(iter(dataloader))\n",
        "helper.imshow(images[0], normalize=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e2c2148960f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'helper' has no attribute 'imshow'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1P_vFsz40yT",
        "colab_type": "text"
      },
      "source": [
        "If you loaded the data correctly, you should see something like this (your image will be different):\n",
        "\n",
        "<img src='https://github.com/leandromouralima/deep-learning/blob/master/intro-to-pytorch/assets/cat_cropped.png?raw=1' width=244>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlVQo_Nc40yU",
        "colab_type": "text"
      },
      "source": [
        "## Data Augmentation\n",
        "\n",
        "A common strategy for training neural networks is to introduce randomness in the input data itself. For example, you can randomly rotate, mirror, scale, and/or crop your images during training. This will help your network generalize as it's seeing the same images but in different locations, with different sizes, in different orientations, etc.\n",
        "\n",
        "To randomly rotate, scale and crop, then flip your images you would define your transforms like this:\n",
        "\n",
        "```python\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.5, 0.5, 0.5], \n",
        "                                                            [0.5, 0.5, 0.5])])\n",
        "```\n",
        "\n",
        "You'll also typically want to normalize images with `transforms.Normalize`. You pass in a list of means and list of standard deviations, then the color channels are normalized like so\n",
        "\n",
        "```input[channel] = (input[channel] - mean[channel]) / std[channel]```\n",
        "\n",
        "Subtracting `mean` centers the data around zero and dividing by `std` squishes the values to be between -1 and 1. Normalizing helps keep the network work weights near zero which in turn makes backpropagation more stable. Without normalization, networks will tend to fail to learn.\n",
        "\n",
        "You can find a list of all [the available transforms here](http://pytorch.org/docs/0.3.0/torchvision/transforms.html). When you're testing however, you'll want to use images that aren't altered (except you'll need to normalize the same way). So, for validation/test images, you'll typically just resize and crop.\n",
        "\n",
        ">**Exercise:** Define transforms for training data and testing data below. Leave off normalization for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTuglZRg40yV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'Cat_Dog_data'\n",
        "\n",
        "# TODO: Define transforms for the training data and testing data\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor()])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(255),\n",
        "                                       transforms.CenterCrop(224),\n",
        "                                       transforms.ToTensor()])\n",
        "\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "j3eCDEZP40yY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "c561e664-d7cf-4f2f-eca5-f738d8bdb31c"
      },
      "source": [
        "# change this to the trainloader or testloader \n",
        "data_iter = iter(testloader)\n",
        "\n",
        "images, labels = next(data_iter)\n",
        "fig, axes = plt.subplots(figsize=(10,4), ncols=4)\n",
        "for ii in range(4):\n",
        "    ax = axes[ii]\n",
        "    helper.imshow(images[ii], ax=ax, normalize=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1116eab69bff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'helper' has no attribute 'imshow'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABL0AAAH4CAYAAACv7J/ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEB3/j8B6xNpPSRt1QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 606,
              "height": 252
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C65h5UI40yd",
        "colab_type": "text"
      },
      "source": [
        "Your transformed images should look something like this.\n",
        "\n",
        "<center>Training examples:</center>\n",
        "<img src='https://github.com/leandromouralima/deep-learning/blob/master/intro-to-pytorch/assets/train_examples.png?raw=1' width=500px>\n",
        "\n",
        "<center>Testing examples:</center>\n",
        "<img src='https://github.com/leandromouralima/deep-learning/blob/master/intro-to-pytorch/assets/test_examples.png?raw=1' width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf5kYlEg40ye",
        "colab_type": "text"
      },
      "source": [
        "At this point you should be able to load data for training and testing. Now, you should try building a network that can classify cats vs dogs. This is quite a bit more complicated than before with the MNIST and Fashion-MNIST datasets. To be honest, you probably won't get it to work with a fully-connected network, no matter how deep. These images have three color channels and at a higher resolution (so far you've seen 28x28 images which are tiny).\n",
        "\n",
        "In the next part, I'll show you how to use a pre-trained network to build a model that can actually solve this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iqGE2c140yf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "48221c6a-13e4-46e7-b984-6fb1760e2903"
      },
      "source": [
        "# Optional TODO: Attempt to build a network to classify cats vs dogs from this dataset\n",
        "display(images.shape)\n",
        "labels.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 224, 224])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyEgKjkN40yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(150528, 16384)\n",
        "        self.fc2 = nn.Linear(16384, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 128)\n",
        "        self.fc4 = nn.Linear(128, 2)\n",
        "\n",
        "        # Dropout module with 0.2 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "        x = F.log_softmax(self.fc4(x), dim=1)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1ZK7LLv40yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Classifier()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "epochs = 3\n",
        "steps = 0\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        log_ps = model(images)\n",
        "        loss = criterion(log_ps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    else:\n",
        "        test_loss = 0\n",
        "        accuracy = 0\n",
        "        \n",
        "        # Turn off gradients for validation, saves memory and computations\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for images, labels in testloader:\n",
        "                log_ps = model(images)\n",
        "                test_loss += criterion(log_ps, labels)\n",
        "                \n",
        "                ps = torch.exp(log_ps)\n",
        "                top_p, top_class = ps.topk(1, dim=1)\n",
        "                equals = top_class == labels.view(*top_class.shape)\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        train_losses.append(running_loss/len(trainloader))\n",
        "        test_losses.append(test_loss/len(testloader))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}